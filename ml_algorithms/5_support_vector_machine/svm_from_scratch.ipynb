{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50fa056a-b644-46ff-8888-418272489a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363aa2fb-77da-489d-aaf9-39c235b19338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine Implementation\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Convert labels from {0, 1} to {-1, 1} for SVM compatibility\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        # Gradient Descent\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) + self.b) >= 1\n",
    "                if condition:\n",
    "                    # Correct classification with margin: Apply regularization only\n",
    "                    dw = 2 * self.lambda_param * self.w\n",
    "                    db = 0\n",
    "                else:\n",
    "                    # Misclassified or within the margin: Update using hinge loss gradient\n",
    "                    dw = 2 * self.lambda_param * self.w - np.dot(x_i, y_[idx])\n",
    "                    db = -y_[idx]\n",
    "\n",
    "                # Update the weights and bias\n",
    "                self.w -= self.learning_rate * dw\n",
    "                self.b -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Calculate the linear combination plus bias and apply the sign function\n",
    "        linear_output = np.dot(X, self.w) + self.b\n",
    "        return np.sign(linear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08765112-4e7e-40b8-b926-6e089026a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-Beta Score\n",
    "\n",
    "def calculate_f_beta_score(y_true, y_pred, beta=1.0):\n",
    "    \"\"\"\n",
    "    Calculate the F-beta score.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): True binary labels.\n",
    "    y_pred (array-like): Predicted binary labels.\n",
    "    beta (float): Weight of recall in the combined score.\n",
    "\n",
    "    Returns:\n",
    "    float: The F-beta score.\n",
    "    \"\"\"\n",
    "    # Calculate True Positives (TP), False Positives (FP), False Negatives (FN)\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Calculate Precision and Recall\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    # Calculate the F-beta score\n",
    "    if precision == 0 and recall == 0:\n",
    "        return 0.0  # To handle the case when both precision and recall are zero\n",
    "    \n",
    "    f_beta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n",
    "    return f_beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "661b4486-296a-4ab9-9b2b-f45fcca5b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple binary classification dataset\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=42)\n",
    "y = np.where(y == 0, 0, 1)  # Convert class labels from 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb50882f-d8a9-4d54-ad6a-84d90456e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86afbbe7-e413-4dca-9b20-0019c0cc02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data to SVM\n",
    "\n",
    "svm = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30d2e04-503b-482a-898b-def71fee86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test data\n",
    "\n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901a3694-3148-41b1-9363-acc626aaea0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate F1 Score\n",
    "\n",
    "calculate_f_beta_score(y_test, y_pred, beta=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
