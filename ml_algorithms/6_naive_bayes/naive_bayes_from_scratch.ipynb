{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d646e108-4981-4f67-80b4-5f36c40861e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a42aa56-40dc-417d-a9cd-0c3d12a5c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Naive Bayes\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.likelihoods = {}\n",
    "        self.classes = None\n",
    "        self.features_count = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Naive Bayes model to the data.\n",
    "        X: 2D array-like of shape (n_samples, n_features)\n",
    "        y: 1D array-like of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        self.features_count = X.shape[1]\n",
    "        self.class_priors = {cls: np.mean(y == cls) for cls in self.classes}\n",
    "        \n",
    "        # Calculate the likelihoods for each feature given each class\n",
    "        self.likelihoods = {cls: {} for cls in self.classes}\n",
    "        for cls in self.classes:\n",
    "            X_cls = X[y == cls]\n",
    "            self.likelihoods[cls]['mean'] = np.mean(X_cls, axis=0)\n",
    "            self.likelihoods[cls]['var'] = np.var(X_cls, axis=0) + 1e-6  # Adding a small value to avoid division by zero\n",
    "\n",
    "    def _calculate_likelihood(self, X, cls):\n",
    "        \"\"\"\n",
    "        Calculate the Gaussian likelihood of the data X for class cls.\n",
    "        \"\"\"\n",
    "        mean = self.likelihoods[cls]['mean']\n",
    "        var = self.likelihoods[cls]['var']\n",
    "        numerator = np.exp(- (X - mean) ** 2 / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def _calculate_posterior(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the posterior probability for each class given the input X.\n",
    "        \"\"\"\n",
    "        posteriors = {}\n",
    "        for cls in self.classes:\n",
    "            prior = np.log(self.class_priors[cls])\n",
    "            likelihood = np.sum(np.log(self._calculate_likelihood(X, cls)))\n",
    "            posteriors[cls] = prior + likelihood\n",
    "        return max(posteriors, key=posteriors.get)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class for each sample in X.\n",
    "        X: 2D array-like of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        return np.array([self._calculate_posterior(x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "485eec49-9a4c-4956-953a-23b1c411274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating synthetic data\n",
    "\n",
    "def generate_synthetic_data(n_samples_per_class=100, n_features=2, mean_0=0, mean_1=2, var_0=1, var_1=1):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for a binary classification problem.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples_per_class: int, number of samples per class\n",
    "    - n_features: int, number of features\n",
    "    - mean_0: float, mean of features for class 0\n",
    "    - mean_1: float, mean of features for class 1\n",
    "    - var_0: float, variance of features for class 0\n",
    "    - var_1: float, variance of features for class 1\n",
    "    \n",
    "    Returns:\n",
    "    - X: 2D numpy array of shape (n_samples, n_features)\n",
    "    - y: 1D numpy array of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # Generate data for class 0\n",
    "    X0 = np.random.normal(loc=mean_0, scale=np.sqrt(var_0), size=(n_samples_per_class, n_features))\n",
    "    y0 = np.zeros(n_samples_per_class)\n",
    "    \n",
    "    # Generate data for class 1\n",
    "    X1 = np.random.normal(loc=mean_1, scale=np.sqrt(var_1), size=(n_samples_per_class, n_features))\n",
    "    y1 = np.ones(n_samples_per_class)\n",
    "    \n",
    "    # Combine the data\n",
    "    X = np.vstack((X0, X1))\n",
    "    y = np.concatenate((y0, y1))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd15a7a-4915-4385-8f1e-9be7967af743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Accuracy\n",
    "\n",
    "def evaluate_f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score of predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: 1D numpy array, the true labels\n",
    "    - y_pred: 1D numpy array, the predicted labels\n",
    "    \n",
    "    Returns:\n",
    "    - f1_score: float, the F1 score of the predictions\n",
    "    \"\"\"\n",
    "    # Calculate True Positives, False Positives, False Negatives\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e740260f-acf4-4567-9412-c0315b180517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating synthetic data\n",
    "\n",
    "X, y = generate_synthetic_data(n_samples_per_class=50, n_features=2, mean_0=0, mean_1=2, var_0=1, var_1=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943a8878-ba6f-47e4-8bfa-bafe865fbeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ad1fad-178d-48d0-b11e-d8942f535bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes classifier\n",
    "\n",
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "302d52a4-7125-4437-a6b1-972cde138121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the training data to the model\n",
    "\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce4ba0a-5e6e-4119-9205-9ac57c132b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the classes\n",
    "\n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fae969fb-0631-4816-8a57-c15e64636303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.88\n",
      "Recall: 0.88\n",
      "F1 Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Evaluate F1 score\n",
    "\n",
    "evaluate_f1_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
