{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Libraries \n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model\n",
    "\n",
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents to embed\n",
    "\n",
    "documents = [\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"The Great Wall of China is one of the Seven Wonders.\",\n",
    "    \"Python is a programming language known for its simplicity.\",\n",
    "    \"AI is transforming industries with machine learning.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the documents\n",
    "\n",
    "document_embeddings = embedder.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a FAISS index\n",
    "\n",
    "embedding_dim = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(np.array(document_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieval\n",
    "\n",
    "def retrieve(query, k=2):\n",
    "    query_embedding = embedder.encode([query])\n",
    "    return retrieve_with_embedding(query_embedding, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieval with embeddings\n",
    "\n",
    "def retrieve_with_embedding(query_embedding, k=2):\n",
    "    query_embedding = query_embedding.reshape(1, 384)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [(documents[idx], round(float(distances[0][i]),2)) for i, idx in enumerate(indices[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e7c21a6a8f43cdbac526229c40a349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "# Initializing LLM\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE System Prompt\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a hypothetical document generator. Given a document your task is to create a Hypothetical Document that might resemble the actual document \n",
    "that should be retrieved where the correct answer would be present.\n",
    "\n",
    "YOU ARE NOT SUPPOSED TO MENTION ANYWHERE IN THE GENERATED DOCUMENT THAT IT IS A HYPOTHETICAL DOCUMENT.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Setup\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, model, tokenizer, system: str = \"\", max_tokens: int = 512) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system = system\n",
    "        self.max_tokens = max_tokens\n",
    "        self.messages: list = []\n",
    "\n",
    "        # Initialize with system message if provided\n",
    "        if self.system:\n",
    "            self.system_prompt_message = {\"role\": \"system\", \"content\": system}\n",
    "\n",
    "    def __call__(self, message: str) -> str:\n",
    "        if message:\n",
    "            self.messages = [self.system_prompt_message, {\"role\": \"user\", \"content\": message}]\n",
    "        \n",
    "        # Execute and get the response\n",
    "        result = self.execute()\n",
    "        return result\n",
    "        \n",
    "    def execute(self) -> str:\n",
    "        \"\"\"Generates a response from the model based on the conversation history.\"\"\"\n",
    "        try:\n",
    "            # Prepare input text and tokenize\n",
    "            text = self.prepare_input_text()\n",
    "            model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "    \n",
    "            # Generate response\n",
    "            generated_ids = self.model.generate(\n",
    "                **model_inputs,\n",
    "                temperature=0.7,\n",
    "                max_new_tokens=self.max_tokens\n",
    "            )\n",
    "    \n",
    "            # Trim input tokens to get only the generated output\n",
    "            generated_ids = [\n",
    "                output_ids[len(input_ids):] \n",
    "                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "    \n",
    "            # Decode the response\n",
    "            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error during execution: {e}\")\n",
    "            return \"Error: Unable to generate a response.\"\n",
    "    \n",
    "    def prepare_input_text(self) -> str:\n",
    "        \"\"\"Prepares the input text using the tokenizer's chat template.\"\"\"\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            self.messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothetical Document Generator\n",
    "\n",
    "def hypothetical_document_generator(query):\n",
    "    HyDE = LLM(model=model, tokenizer=tokenizer, system=system_prompt)\n",
    "    docs = []\n",
    "    for i in range(5):\n",
    "        docs.append(HyDE(query))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "\n",
    "query = \"Where is the Eiffel Tower located?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Hypothetical Documents\n",
    "\n",
    "docs = hypothetical_document_generator(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings for Hypothetical Documents\n",
    "\n",
    "hypothetical_embeddings = embedder.encode(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average embedding representation of Hypothetical Documents\n",
    "\n",
    "hyde = np.mean(hypothetical_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Eiffel Tower is located in Paris.', 25.29)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normal Retrieval Result\n",
    "\n",
    "retrieved_docs = retrieve(query, k=1)\n",
    "retrieved_docs[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Eiffel Tower is located in Paris.', 18.28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HyDE Retrieval Result\n",
    "\n",
    "retrieved_docs = retrieve_with_embedding(hyde, k=1)\n",
    "retrieved_docs[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "18af0237fee34a7bbfdcf9fe13d4775c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "20fc676d9e3c4f9b9a393302c89b9a78": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "62e7c21a6a8f43cdbac526229c40a349": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_718ff4435ad64f568ea9c7f141b3080f",
        "IPY_MODEL_9c954c7e4a804d0dad9658192270fa88",
        "IPY_MODEL_d2dd52197a2449ae8d9591319c5aaec9"
       ],
       "layout": "IPY_MODEL_20fc676d9e3c4f9b9a393302c89b9a78"
      }
     },
     "718ff4435ad64f568ea9c7f141b3080f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7bbafc3e10b940819bbd53fb53e8223e",
       "style": "IPY_MODEL_9cfe77d3f88b4c83b039659188768ab2",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "7bbafc3e10b940819bbd53fb53e8223e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "896a176e75cb4c10b10b0b474875cdfc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9c954c7e4a804d0dad9658192270fa88": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_896a176e75cb4c10b10b0b474875cdfc",
       "max": 4,
       "style": "IPY_MODEL_edef538cb9ab429ba7b3d36164837ad6",
       "value": 4
      }
     },
     "9cfe77d3f88b4c83b039659188768ab2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2dd52197a2449ae8d9591319c5aaec9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_18af0237fee34a7bbfdcf9fe13d4775c",
       "style": "IPY_MODEL_ed4bc38b65cd4a68b55be6711c2c1c4d",
       "value": " 4/4 [00:02&lt;00:00,  1.48it/s]"
      }
     },
     "ed4bc38b65cd4a68b55be6711c2c1c4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "edef538cb9ab429ba7b3d36164837ad6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
