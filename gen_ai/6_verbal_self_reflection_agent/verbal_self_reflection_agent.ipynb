{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786b01b6-9845-4d78-9a24-3c1dd4c9c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40bbc617-5ce0-4156-875a-3263c3ff7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct Agent Loop Components \n",
    "\n",
    "# Thought (Reason)\n",
    "# Action (Tools)\n",
    "# Observation (Tool Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5d6240-02d4-4975-af56-4a1926a525de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fcd36ce0db4f9a9c937fa3347edc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "# Initializing LLM\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7651b1c5-b38d-4885-a84c-f9281662715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt for the Agent\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are to proceed one step at a time and stop after each step. Each step should be completed individually. After generating \"THOUGHT,\" \"GENERATION,\" or \"REFLECTION,\" you should **WAIT** until prompted to continue.\n",
    "\n",
    "**Process**:\n",
    "1. Start with **THOUGHT:** to describe the next logical step. WAIT afterward.\n",
    "2. Proceed to **GENERATION:** if prompted, using the previous thought. WAIT afterward.\n",
    "3. Move to **REFLECTION:** to critique the generation if prompted. WAIT afterward.\n",
    "4. End with **ANSWER:** only when prompted and confident that the final answer is ready.\n",
    "\n",
    "**DO NOT proceed automatically** to the next step without waiting. Each step must end with **WAIT.**\n",
    "\n",
    "Examples:\n",
    "\n",
    "**Query:** \"What is the capital of France?\"\n",
    "\n",
    "THOUGHT: I need to recall the capital city of France.\n",
    "WAIT\n",
    "\n",
    "**Prompt to continue**\n",
    "\n",
    "GENERATION: The capital of France is Paris.\n",
    "WAIT\n",
    "\n",
    "**Prompt to continue**\n",
    "\n",
    "REFLECTION: The output states \"Paris,\" which is correct.\n",
    "WAIT\n",
    "\n",
    "**Prompt to continue**\n",
    "\n",
    "ANSWER: Paris\n",
    "\n",
    "**Query:** \"Solve 8 + 15.\"\n",
    "\n",
    "THOUGHT: I should perform the addition of 8 and 15.\n",
    "WAIT\n",
    "\n",
    "**Prompt to continue**\n",
    "\n",
    "GENERATION: The result of 8 + 15 is 23.\n",
    "WAIT\n",
    "\n",
    "**Prompt to continue**\n",
    "\n",
    "REFLECTION: The result is 23, which is correct.\n",
    "WAIT\n",
    "\n",
    "**Prompt to continue**\n",
    "\n",
    "ANSWER: 23\n",
    "\n",
    "Now, proceed with one step at a time, using **WAIT** after each.\n",
    "\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d93ea17-b0f6-4af2-84f1-0ccfe132a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbal Self Reflection Based Agent \n",
    "\n",
    "class SelfReflectionAgent:\n",
    "    def __init__(self, model, tokenizer, system: str = \"\", max_tokens: int = 512) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system = system\n",
    "        self.max_tokens = max_tokens\n",
    "        self.messages: list = []\n",
    "        \n",
    "        # Initialize with system message if provided\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    def __call__(self, message: str) -> str:\n",
    "        if message:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        # Execute and get the response\n",
    "        result = self.execute()\n",
    "        \n",
    "        # Append assistant's response to message history\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "    def execute(self) -> str:\n",
    "        \"\"\"Generates the next response from the model based on the conversation history.\"\"\"\n",
    "        try:\n",
    "            # Prepare input text and tokenize\n",
    "            text = self.prepare_input_text()\n",
    "            model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "            # Generate response\n",
    "            generated_ids = self.model.generate(\n",
    "                **model_inputs,\n",
    "                temperature=0.1,\n",
    "                max_new_tokens=self.max_tokens\n",
    "            )\n",
    "\n",
    "            # Trim input tokens to get only the generated output\n",
    "            generated_ids = [\n",
    "                output_ids[len(input_ids):] \n",
    "                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "\n",
    "            # Decode the response\n",
    "            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error during execution: {e}\")\n",
    "            return \"Error: Unable to generate a response.\"\n",
    "\n",
    "    def prepare_input_text(self) -> str:\n",
    "        \"\"\"Prepares the input text using the tokenizer's chat template.\"\"\"\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            self.messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1864d65c-8613-4570-a18c-c3ec03e71ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent with the model, tokenizer, and system prompt\n",
    "\n",
    "agent = SelfReflectionAgent(model=model, tokenizer=tokenizer, system=system_prompt, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc08045-da22-4422-ad5e-77492762070b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THOUGHT: First, I need to know the diameter of the Earth. The average diameter of the Earth is approximately 12,742 kilometers. Then, I will square this value and multiply it by 3.95.\\nWAIT'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_query = \"What is 3.95 times diameter of earth squared?\"\n",
    "\n",
    "agent(initial_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05626450-a694-458a-9e0d-7c5761322987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GENERATION: The diameter of the Earth is approximately 12,742 kilometers. Squaring this value gives \\\\(12,742^2 = 162,310,564\\\\) square kilometers. Multiplying this by 3.95 gives \\\\(3.95 \\\\times 162,310,564 = 641,227,245.8\\\\) kilometers squared.\\nWAIT'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent('proceed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f033603e-8983-44df-807c-8bf54adbdb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"REFLECTION: The calculation seems correct, but the units should be consistent. Since we are dealing with a physical quantity (diameter in kilometers), the result should also be in a relevant unit. However, the problem does not specify the unit for the final answer, so I'll provide the numerical value.\\nWAIT\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent('proceed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d3f94e-ab71-4410-ae23-6d6e02375bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = agent('proceed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a24f362-15a8-4f1b-86c3-ebcaf24f6677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.startswith(\"ANSWER:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be66b35e-d8e7-4599-b7f0-e28116c17a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_loop(query, max_llm_calls=5):\n",
    "    response = agent(initial_query)\n",
    "    llm_calls = 0\n",
    "    while llm_calls < max_llm_calls:\n",
    "        if response.startswith(\"ANSWER:\"):\n",
    "            return response\n",
    "        else:\n",
    "            llm_calls += 1\n",
    "            response = agent(\"proceed\")\n",
    "            print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6856c38a-ed88-4eb4-91fa-3599f6848c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATION: The diameter of the Earth is approximately 12,742 kilometers. Squaring this value gives \\(12,742^2 = 162,310,564\\) square kilometers. Multiplying this by 3.95 gives \\(3.95 \\times 162,310,564 = 641,227,245.8\\) kilometers squared.\n",
      "WAIT\n",
      "REFLECTION: The calculation seems accurate, but it's important to ensure the units are consistent and the result is correctly interpreted. Since we're dealing with a multiplication of a dimensionless constant (3.95) with a squared distance (diameter squared), the result is in the same units as the squared diameter.\n",
      "WAIT\n",
      "ANSWER: 641,227,245.8\n"
     ]
    }
   ],
   "source": [
    "agent = SelfReflectionAgent(model=model, tokenizer=tokenizer, system=system_prompt, max_tokens=512)\n",
    "answer = run_agent_loop(initial_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe72487d-99f2-4bcb-af48-415fb2c9c9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANSWER: 641,227,245.8'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04361e8e812c4480914a257223521cda": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4075f34e1275419fb5cb08d76b69dcdb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "60aaa8e35e4c4b5bacf97021d4cff925": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4075f34e1275419fb5cb08d76b69dcdb",
       "style": "IPY_MODEL_04361e8e812c4480914a257223521cda",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "6780cd89e6214556844803a38dc16bff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "751e91fc97e9424f8330145ce71af394": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8af805c3a9c94affaae6149e042ea6ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9b03fa57e52b4ba3b11418e69cf63313": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_f6f99f1664ec4d73808ee9d2d3614974",
       "max": 4,
       "style": "IPY_MODEL_6780cd89e6214556844803a38dc16bff",
       "value": 4
      }
     },
     "a014efba6de84371b8358e73725d254f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8af805c3a9c94affaae6149e042ea6ea",
       "style": "IPY_MODEL_751e91fc97e9424f8330145ce71af394",
       "value": " 4/4 [00:02&lt;00:00,  1.64it/s]"
      }
     },
     "a6e68650efa64695982ac13c72b1383a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f3fcd36ce0db4f9a9c937fa3347edc31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_60aaa8e35e4c4b5bacf97021d4cff925",
        "IPY_MODEL_9b03fa57e52b4ba3b11418e69cf63313",
        "IPY_MODEL_a014efba6de84371b8358e73725d254f"
       ],
       "layout": "IPY_MODEL_a6e68650efa64695982ac13c72b1383a"
      }
     },
     "f6f99f1664ec4d73808ee9d2d3614974": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
