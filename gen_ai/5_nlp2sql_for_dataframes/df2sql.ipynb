{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296f7ab1-62f3-4b1a-8b92-7e2c0fd3788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9faa667-4f08-4a3e-b4c8-36cf647b17bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting GPU \n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c86e39-e7af-49cb-8a46-4bcc50050548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaeffa010e44a18a4e368cd969b57b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "# Initializing LLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ea4dc8-b086-4356-ab9b-8280d7d8ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dynamic DataFrame\n",
    "\n",
    "data = {\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'age': [25, 30, 35],\n",
    "    'department': ['sales', 'marketing', 'sales']\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff0a539-d060-48ae-83e6-c8e52a5f3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(model, tokenizer, prompt):\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are a SQL generation assistant. You will receive:\n",
    "    1. A question written in natural language.\n",
    "    2. A database schema with table names and columns.\n",
    "\n",
    "    Your task is to generate an accurate and valid SQL query based on the question, strictly adhering to the following guidelines:\n",
    "\n",
    "    Instructions for SQL Query Generation:\n",
    "    - Output Format: Output only the SQL query without any extra text or explanation.\n",
    "    - Schema Matching: Use only the table names and columns specified in the schema provided. Do not assume or invent any columns.\n",
    "    - Table and Column Names: Refer to all table and column names exactly as they appear in the schema, including case-sensitivity if specified.\n",
    "    - Joins and Relationships: If the query requires data from multiple tables, assume natural joins where primary and foreign key relationships are implied.\n",
    "    - Conditions: Use WHERE clauses for conditions specified in the question. Include logical operators (AND, OR) if multiple conditions are mentioned.\n",
    "    - Aggregations: For questions involving calculations (e.g., total, average, count), use appropriate aggregate functions (e.g., SUM, AVG, COUNT) only if specifically mentioned or implied.\n",
    "    - Ordering and Limiting: If the question implies sorting (e.g., \"highest,\" \"lowest,\" \"most recent\"), use ORDER BY. Use LIMIT only if a specific number of results is requested or implied.\n",
    "    - Output Columns: Include only the columns relevant to answering the question. If no specific columns are requested, return all columns.\n",
    "\n",
    "    Error Handling:\n",
    "    - If any column or table required by the question is missing in the schema, do not proceed. Instead, return an error message: Error: Column or table not found in schema.\n",
    "\n",
    "    Examples:\n",
    "    1. Schema:\n",
    "       - Table: employees\n",
    "         - id (INTEGER)\n",
    "         - name (TEXT)\n",
    "         - age (INTEGER)\n",
    "         - department (TEXT)\n",
    "\n",
    "       Question: \"List the names of employees who are older than 30.\"\n",
    "       Output: SELECT name FROM employees WHERE age > 30;\n",
    "\n",
    "    2. Schema:\n",
    "       - Table: sales\n",
    "         - id (INTEGER)\n",
    "         - product_name (TEXT)\n",
    "         - amount (FLOAT)\n",
    "         - sale_date (DATE)\n",
    "\n",
    "       Question: \"Find the total sales amount.\"\n",
    "       Output: SELECT SUM(amount) FROM sales;\n",
    "\n",
    "    Provide only the SQL query.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response= tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43884509-4d94-4e63-87fe-0c7f488503e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to dynamically get schema from DataFrame\n",
    "\n",
    "def get_dataframe_schema(df):\n",
    "    schema = \"Table: dynamic_table\\n\"\n",
    "    for column, dtype in df.dtypes.items():\n",
    "        schema += f\"- {column} ({dtype.name.upper()})\\n\"\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42184b18-23d5-430b-a540-d7059de4d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NL to SQL using an LLM\n",
    "\n",
    "def nl_to_sql(model, tokenizer, nl_question, schema):\n",
    "    prompt = f\"Convert the following question into an SQL query.\\n\\nSchema:\\n{schema}\\n\\nQuestion: \\\"{nl_question}\\\"\\n\\nSQL:\"\n",
    "    sql_query = get_llm_response(model, tokenizer, prompt)\n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe4d896-15b9-4b7e-9995-9269a6279517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural language question\n",
    "\n",
    "nl_question = \"Get the names of employees older than 30 who work in the sales department.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c31b8c4b-42cb-4c96-bd9a-36ea282232a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Schema:\n",
      " Table: dynamic_table\n",
      "- id (INT64)\n",
      "- name (OBJECT)\n",
      "- age (INT64)\n",
      "- department (OBJECT)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate schema dynamically\n",
    "\n",
    "schema = get_dataframe_schema(df)\n",
    "print(\"Generated Schema:\\n\", schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b43f3f2-657e-4d9a-b723-50419bc91c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SQL Query: SELECT name FROM dynamic_table WHERE age > 30 AND department = 'sales';\n"
     ]
    }
   ],
   "source": [
    "# Generate SQL query\n",
    "\n",
    "sql_query = nl_to_sql(model, tokenizer, nl_question, schema)\n",
    "print(\"Generated SQL Query:\", sql_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2f0de6827512410d82a87c759354e2c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "333bf915ace34a1789d26312708f038e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_52115b1ea5ab404aa9fc89218464172c",
       "style": "IPY_MODEL_cd213744523b44e2a5cc7c3a235a1d87",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "350f34f308374efe9ee225aed60387f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3c0d443989904d88a5b87f23adeb0de8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "52115b1ea5ab404aa9fc89218464172c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6d04d435c6784cfdb892e2cfdba933b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "903a3127af1c4538b11808728058e921": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_2f0de6827512410d82a87c759354e2c4",
       "max": 4,
       "style": "IPY_MODEL_6d04d435c6784cfdb892e2cfdba933b8",
       "value": 4
      }
     },
     "94c0c7a494b144adbe8f084e47ddd775": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "98e7d2bb48ff4e1abfc9a839534cafed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3c0d443989904d88a5b87f23adeb0de8",
       "style": "IPY_MODEL_350f34f308374efe9ee225aed60387f2",
       "value": " 4/4 [00:02&lt;00:00,  1.52it/s]"
      }
     },
     "adaeffa010e44a18a4e368cd969b57b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_333bf915ace34a1789d26312708f038e",
        "IPY_MODEL_903a3127af1c4538b11808728058e921",
        "IPY_MODEL_98e7d2bb48ff4e1abfc9a839534cafed"
       ],
       "layout": "IPY_MODEL_94c0c7a494b144adbe8f084e47ddd775"
      }
     },
     "cd213744523b44e2a5cc7c3a235a1d87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
