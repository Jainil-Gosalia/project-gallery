{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786b01b6-9845-4d78-9a24-3c1dd4c9c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40bbc617-5ce0-4156-875a-3263c3ff7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct Agent Loop Components \n",
    "\n",
    "# Thought (Reason)\n",
    "# Action (Tools)\n",
    "# Observation (Tool Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5d6240-02d4-4975-af56-4a1926a525de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ced3b1b044468e932b87d396c11c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "# Initializing LLM\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7651b1c5-b38d-4885-a84c-f9281662715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt for the Agent\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are to proceed one step at a time and stop after each step. Each step should be completed individually. After generating \"THOUGHT,\" \"GENERATION,\" or \"REFLECTION,\" you should **WAIT** until prompted to continue.\n",
    "\n",
    "**Process**:\n",
    "1. Start with **THOUGHT:** to describe the next logical step. WAIT afterward.\n",
    "2. Proceed to **GENERATION:** if prompted, using the previous thought. WAIT afterward.\n",
    "3. Move to **REFLECTION:** to rate the output on a **scale from 1 to 10** based on its quality or correctness (1 = very poor, 10 = perfect). Only provide the numeric score with no additional text. WAIT afterward.\n",
    "4. End with **ANSWER:** only when prompted and confident that the final answer is ready.\n",
    "\n",
    "**DO NOT proceed automatically** to the next step without waiting. Each step must end with **WAIT.**\n",
    "\n",
    "Examples:\n",
    "\n",
    "**Query:** \"What is the capital of France?\"\n",
    "\n",
    "THOUGHT: I need to recall the capital city of France.\n",
    "WAIT\n",
    "\n",
    "**Prompt to continue**\n",
    "\n",
    "GENERATION: The capital of France is Paris.\n",
    "WAIT\n",
    "\n",
    "**Prompt to continue**\n",
    "\n",
    "REFLECTION: 10\n",
    "WAIT\n",
    "\n",
    "**Prompt to continue**\n",
    "\n",
    "ANSWER: Paris\n",
    "\n",
    "**Query:** \"Solve 8 + 15.\"\n",
    "\n",
    "THOUGHT: I should perform the addition of 8 and 15.\n",
    "WAIT\n",
    "\n",
    "**Prompt to continue**\n",
    "\n",
    "GENERATION: The result of 8 + 15 is 23.\n",
    "WAIT\n",
    "\n",
    "**Prompt to continue**\n",
    "\n",
    "REFLECTION: 10\n",
    "WAIT\n",
    "\n",
    "**Prompt to continue**\n",
    "\n",
    "ANSWER: 23\n",
    "\n",
    "Now, proceed with one step at a time, using **WAIT** after each.\n",
    "\"\"\".strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d93ea17-b0f6-4af2-84f1-0ccfe132a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Self Reflection Based Agent \n",
    "\n",
    "class SelfReflectionAgent:\n",
    "    def __init__(self, model, tokenizer, system: str = \"\", max_tokens: int = 512) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system = system\n",
    "        self.max_tokens = max_tokens\n",
    "        self.messages: list = []\n",
    "        \n",
    "        # Initialize with system message if provided\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    def __call__(self, message: str) -> str:\n",
    "        if message:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        # Execute and get the response\n",
    "        result = self.execute()\n",
    "        \n",
    "        # Append assistant's response to message history\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "    def execute(self) -> str:\n",
    "        \"\"\"Generates the next response from the model based on the conversation history.\"\"\"\n",
    "        try:\n",
    "            # Prepare input text and tokenize\n",
    "            text = self.prepare_input_text()\n",
    "            model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "            # Generate response\n",
    "            generated_ids = self.model.generate(\n",
    "                **model_inputs,\n",
    "                temperature=0.1,\n",
    "                max_new_tokens=self.max_tokens\n",
    "            )\n",
    "\n",
    "            # Trim input tokens to get only the generated output\n",
    "            generated_ids = [\n",
    "                output_ids[len(input_ids):] \n",
    "                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "\n",
    "            # Decode the response\n",
    "            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error during execution: {e}\")\n",
    "            return \"Error: Unable to generate a response.\"\n",
    "\n",
    "    def prepare_input_text(self) -> str:\n",
    "        \"\"\"Prepares the input text using the tokenizer's chat template.\"\"\"\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            self.messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be66b35e-d8e7-4599-b7f0-e28116c17a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_loop(query, max_llm_calls=5):\n",
    "    response = agent(initial_query)\n",
    "    llm_calls = 0\n",
    "    # print(response)\n",
    "    while llm_calls < max_llm_calls:\n",
    "        if response.startswith(\"REFLECTION:\"):\n",
    "            if int(response.split(\"REFLECTION:\")[-1].strip()[:2]) < 8:\n",
    "                agent(\"proceed to thought\")\n",
    "            else:\n",
    "                agent(\"proceed to answer\")\n",
    "        if response.startswith(\"ANSWER:\"):\n",
    "            return response\n",
    "        else:\n",
    "            llm_calls += 1\n",
    "            response = agent(\"proceed\")\n",
    "            print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6856c38a-ed88-4eb4-91fa-3599f6848c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATION: The Earth's diameter is approximately 12,742 kilometers. Squaring this value gives \\(12,742^2 = 162,312,564\\) square kilometers. Now, multiplying this by 3.95 gives \\(3.95 \\times 162,312,564 = 641,226,545.8\\) square kilometers.\n",
      "WAIT\n",
      "REFLECTION: 10\n",
      "WAIT\n",
      "ANSWER: 641,226,545.8\n"
     ]
    }
   ],
   "source": [
    "# Running Query\n",
    "\n",
    "initial_query = \"What is 3.95 times diameter of earth squared?\"\n",
    "\n",
    "agent = SelfReflectionAgent(model=model, tokenizer=tokenizer, system=system_prompt, max_tokens=512)\n",
    "answer = run_agent_loop(initial_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe72487d-99f2-4bcb-af48-415fb2c9c9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANSWER: 641,226,545.8'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0649506d7d3541be97a8f21d0859dbad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "208a1945e5d34a30af438f5651e4875c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_91b64d47a31a413f8ce353bc1b7c8001",
       "style": "IPY_MODEL_b364608fe96145ea8db2ca8e0ddf9cee",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "42cd6e6dd82c4b2ab1d2f0e53ba96559": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5bb181a6ec3e42ff8aff169d52bdd254": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a7a5a1b414054d92858a818ff8b63ab4",
       "style": "IPY_MODEL_42cd6e6dd82c4b2ab1d2f0e53ba96559",
       "value": " 4/4 [00:02&lt;00:00,  1.60it/s]"
      }
     },
     "6ae4d921000641718036a638a8dd147a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "74d048c32b8a4efebb4814eb2c848791": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_0649506d7d3541be97a8f21d0859dbad",
       "max": 4,
       "style": "IPY_MODEL_8c683c3298e44389a00818f6104cbec1",
       "value": 4
      }
     },
     "8c683c3298e44389a00818f6104cbec1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "91b64d47a31a413f8ce353bc1b7c8001": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "92ced3b1b044468e932b87d396c11c81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_208a1945e5d34a30af438f5651e4875c",
        "IPY_MODEL_74d048c32b8a4efebb4814eb2c848791",
        "IPY_MODEL_5bb181a6ec3e42ff8aff169d52bdd254"
       ],
       "layout": "IPY_MODEL_6ae4d921000641718036a638a8dd147a"
      }
     },
     "a7a5a1b414054d92858a818ff8b63ab4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b364608fe96145ea8db2ca8e0ddf9cee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
