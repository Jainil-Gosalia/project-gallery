{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61effed5-1c9f-40d0-bf0c-209be4656109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f588a1-ee0a-4ce3-b48f-df2c6d51fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load dataset and build vocab\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "train_texts = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']\n",
    "\n",
    "test_texts = dataset['test']['text']\n",
    "test_labels = dataset['test']['label']\n",
    "\n",
    "def build_vocab(texts, max_size=100000):\n",
    "    words = []\n",
    "    for text in texts:\n",
    "        words.extend(text.lower().split())\n",
    "    freq = Counter(words)\n",
    "    vocab = {word: i+1 for i, (word, _) in enumerate(freq.most_common(max_size))}\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0df4ea-521e-4b9a-a5ef-036439fc5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Encode texts to fixed-length sequences\n",
    "def encode(text, vocab, max_len=100):\n",
    "    tokens = text.lower().split()\n",
    "    idxs = [vocab.get(token, 0) for token in tokens]  # 0 for unknown words\n",
    "    if len(idxs) < max_len:\n",
    "        idxs += [0] * (max_len - len(idxs))\n",
    "    else:\n",
    "        idxs = idxs[:max_len]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e23dd477-0cbc-4746-8909-68e4b009827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(encode(self.texts[idx], self.vocab), dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels, vocab)\n",
    "test_dataset = TextDataset(test_texts, test_labels, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2500ddc1-340e-4f9f-b279-88d177a49c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim=50,\n",
    "        hidden_dim=64,\n",
    "        output_dim=2,\n",
    "        num_layers=2,\n",
    "        dropout=0.3,\n",
    "        bidirectional=True,\n",
    "        padding_idx=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)  # (batch, seq_len, hidden_dim * num_directions)\n",
    "        out = out.mean(dim=1)  # (batch, hidden_dim * num_directions)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "\n",
    "model = RNNClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=128,\n",
    "    hidden_dim=64,\n",
    "    output_dim=2,\n",
    "    num_layers=10,\n",
    "    dropout=0.3,\n",
    "    bidirectional=True,\n",
    "    padding_idx=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b25a95e-b42b-4bc0-9ac0-68a10aec9502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([64, 2])\n",
      "Label shape: torch.Size([64])\n",
      "Sample labels: tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "\n",
    "sample_x, sample_y = next(iter(train_loader))\n",
    "sample_out = model(sample_x.to(\"cpu\"))\n",
    "print(\"Output shape:\", sample_out.shape)\n",
    "print(\"Label shape:\", sample_y.shape)\n",
    "print(\"Sample labels:\", sample_y[:10])\n",
    "\n",
    "# Double Checking for my own Sanity\n",
    "\n",
    "for x_batch, y_batch in train_loader:\n",
    "    y_batch = y_batch.to(\"cpu\")\n",
    "    assert y_batch.min() >= 0 and y_batch.max() < 2, f\"Label values out of range: min={y_batch.min()}, max={y_batch.max()}\"\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcbaf341-f3a8-40fd-84a9-fa7bcb782d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loss and optimizer and scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b69bdeb8-f914-4d34-acf0-d6beaf52aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Training loop\n",
    "def train_epoch(model, loader, optimizer, criterion, device, max_grad_norm=5.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # âœ… Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a581c681-f02f-474e-8c0d-137d6d49e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Evaluation function\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ce7179-13bb-49d4-9f68-37ec1dea2409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=0.6124, Val loss=0.5140, Val acc=0.7519\n",
      "Epoch 2: Train loss=0.3995, Val loss=0.5153, Val acc=0.7626\n",
      "Epoch 3: Train loss=0.2882, Val loss=0.6004, Val acc=0.7654\n",
      "Epoch 4: Train loss=0.2312, Val loss=0.6514, Val acc=0.7626\n",
      "Epoch 5: Train loss=0.1916, Val loss=0.6159, Val acc=0.7587\n",
      "Epoch 6: Train loss=0.1411, Val loss=0.6834, Val acc=0.7577\n",
      "Epoch 7: Train loss=0.1078, Val loss=0.7233, Val acc=0.7704\n",
      "Epoch 8: Train loss=0.0761, Val loss=0.8785, Val acc=0.7636\n",
      "Epoch 9: Train loss=0.0557, Val loss=1.0701, Val acc=0.7680\n",
      "Epoch 10: Train loss=0.0388, Val loss=1.1157, Val acc=0.7702\n",
      "Epoch 11: Train loss=0.0276, Val loss=1.2663, Val acc=0.7694\n",
      "Epoch 12: Train loss=0.0206, Val loss=1.3909, Val acc=0.7688\n",
      "Epoch 13: Train loss=0.0155, Val loss=1.6937, Val acc=0.7634\n",
      "Epoch 14: Train loss=0.0109, Val loss=1.6909, Val acc=0.7646\n",
      "Epoch 15: Train loss=0.0087, Val loss=1.7479, Val acc=0.7638\n",
      "Epoch 16: Train loss=0.0064, Val loss=1.8795, Val acc=0.7638\n",
      "Epoch 17: Train loss=0.0055, Val loss=1.9329, Val acc=0.7634\n",
      "Epoch 18: Train loss=0.0039, Val loss=2.0022, Val acc=0.7643\n",
      "Epoch 19: Train loss=0.0037, Val loss=2.0088, Val acc=0.7650\n",
      "Epoch 20: Train loss=0.0032, Val loss=2.0799, Val acc=0.7650\n",
      "Epoch 21: Train loss=0.0028, Val loss=2.1292, Val acc=0.7646\n",
      "Epoch 22: Train loss=0.0027, Val loss=2.1908, Val acc=0.7636\n",
      "Epoch 23: Train loss=0.0023, Val loss=2.2134, Val acc=0.7629\n",
      "Epoch 24: Train loss=0.0021, Val loss=2.2454, Val acc=0.7637\n",
      "Epoch 25: Train loss=0.0022, Val loss=2.2476, Val acc=0.7639\n",
      "Epoch 26: Train loss=0.0019, Val loss=2.2575, Val acc=0.7641\n",
      "Epoch 27: Train loss=0.0019, Val loss=2.2786, Val acc=0.7637\n",
      "Epoch 28: Train loss=0.0018, Val loss=2.2784, Val acc=0.7644\n",
      "Epoch 29: Train loss=0.0018, Val loss=2.2904, Val acc=0.7644\n",
      "Epoch 30: Train loss=0.0018, Val loss=2.2885, Val acc=0.7644\n",
      "Epoch 31: Train loss=0.0016, Val loss=2.3059, Val acc=0.7639\n",
      "Epoch 32: Train loss=0.0017, Val loss=2.3137, Val acc=0.7644\n",
      "Epoch 33: Train loss=0.0015, Val loss=2.3223, Val acc=0.7645\n",
      "Epoch 34: Train loss=0.0016, Val loss=2.3303, Val acc=0.7647\n",
      "Epoch 35: Train loss=0.0015, Val loss=2.3326, Val acc=0.7645\n",
      "Epoch 36: Train loss=0.0017, Val loss=2.3408, Val acc=0.7640\n",
      "Epoch 37: Train loss=0.0014, Val loss=2.3399, Val acc=0.7640\n",
      "Epoch 38: Train loss=0.0016, Val loss=2.3425, Val acc=0.7638\n",
      "Epoch 39: Train loss=0.0015, Val loss=2.3452, Val acc=0.7640\n",
      "Epoch 40: Train loss=0.0015, Val loss=2.3475, Val acc=0.7641\n",
      "Epoch 41: Train loss=0.0016, Val loss=2.3483, Val acc=0.7644\n",
      "Epoch 42: Train loss=0.0016, Val loss=2.3486, Val acc=0.7641\n",
      "Epoch 43: Train loss=0.0015, Val loss=2.3489, Val acc=0.7642\n",
      "Epoch 44: Train loss=0.0015, Val loss=2.3499, Val acc=0.7643\n",
      "Epoch 45: Train loss=0.0014, Val loss=2.3499, Val acc=0.7643\n",
      "Epoch 46: Train loss=0.0015, Val loss=2.3499, Val acc=0.7644\n",
      "Epoch 47: Train loss=0.0016, Val loss=2.3504, Val acc=0.7644\n",
      "Epoch 48: Train loss=0.0014, Val loss=2.3510, Val acc=0.7644\n",
      "Epoch 49: Train loss=0.0014, Val loss=2.3513, Val acc=0.7642\n",
      "Epoch 50: Train loss=0.0015, Val loss=2.3513, Val acc=0.7642\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "early_stopper = EarlyStopper(patience=5)\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train loss={train_loss:.4f}, Val loss={val_loss:.4f}, Val acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "743f0113-6af1-47e1-8299-d3e391fae283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.42%\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Accuracy: {round(test_acc[1] * 100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
